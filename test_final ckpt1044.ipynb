{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thiendc/InferConverRec/src\n"
     ]
    }
   ],
   "source": [
    "%cd src\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "from collections import defaultdict\n",
    "from accelerate import Accelerator\n",
    "import json\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from refinement import *\n",
    "from generator.conv import GenerateConversation\n",
    "from generator.rec import GenerateRecommendation\n",
    "from generator.hooks import hook_sentences ,promotional_sentences, pick_verbs\n",
    "from fuzzywuzzy import fuzz\n",
    "multiprocessing.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities_and_ids(json_file_path):\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    entity_id = {}\n",
    "    id_entity = {}\n",
    "    for k, v in data.items():\n",
    "        k = k.split(\"/\")[-1]\n",
    "        k = k.replace(\">\", \"\")\n",
    "        # if \"(\" and \")\" in k:\n",
    "        #     k=re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", k)\n",
    "        k = k.replace(\"_\", \" \")\n",
    "        k = k.strip()\n",
    "        # k = k.lower()\n",
    "\n",
    "        entity_id[k] = v\n",
    "        id_entity[v] = k\n",
    "\n",
    "    return entity_id\n",
    "\n",
    "def cosine_similarity_entities(entities):\n",
    "    matched_entities = []\n",
    "    entity_names = list(get_entities_and_ids('./data/redial_gen/entity2id.json').keys())\n",
    "    \n",
    "    # Tạo vector TF-IDF cho tất cả thực thể trong entity_names và entities\n",
    "    vectorizer = TfidfVectorizer().fit(entity_names + entities)\n",
    "    \n",
    "    # Vector hóa danh sách entity\n",
    "    entity_vectors = vectorizer.transform(entities)\n",
    "    entity_name_vectors = vectorizer.transform(entity_names)\n",
    "\n",
    "    # Duyệt qua từng thực thể\n",
    "    for entity_vector in entity_vectors:\n",
    "        # Tính cosine similarity giữa entity hiện tại và tất cả entity_names\n",
    "        similarities = cosine_similarity(entity_vector, entity_name_vectors).flatten()\n",
    "\n",
    "        # Sắp xếp theo độ tương đồng giảm dần\n",
    "        sorted_similarities = sorted(zip(entity_names, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Lấy top 10 kết quả có độ tương đồng > 0.5 (50%)\n",
    "        top_matches = [entity_name for entity_name, similarity in sorted_similarities[:10] if similarity > 0.5]\n",
    "        matched_entities.extend(top_matches)\n",
    "\n",
    "    return matched_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input2jsonl(input_str, entity2id_json):\n",
    "    save_path = './data/redial/sample_input_data_processed.jsonl'\n",
    "    is_exist = os.path.exists(save_path)\n",
    "\n",
    "    current_str = ''\n",
    "\n",
    "    if is_exist:\n",
    "        with open(save_path, 'r') as json_file:\n",
    "            current_str = json.load(json_file)\n",
    "            current_str['context'].append(input_str)\n",
    "    else:\n",
    "        current_str = {\"context\": [input_str], \"resp\": \"\", \"rec\": [], \"entity\": []}\n",
    "    \n",
    "    entities = re.findall(r'\\$.*?\\$', input_str)\n",
    "    if entities:\n",
    "        entities = [entity.replace('$', '').lower() for entity in entities]\n",
    "        # list_entities = set(get_entities_and_ids(entity2id_json))\n",
    "\n",
    "        matched_entities = cosine_similarity_entities(entities)\n",
    "        entity_id = get_entities_and_ids('./data/redial_gen/entity2id.json')\n",
    "        if matched_entities:\n",
    "\n",
    "            entity_ids = [entity_id[entity] for entity in matched_entities if entity in get_entities_and_ids(entity2id_json)]\n",
    "            print(\"-------------------------------------------------------------------\")\n",
    "            print(f\"Movies that you {random.choice(pick_verbs)} earlier: {', '.join(matched_entities)}\")\n",
    "            print(f\"Corresponding IDs : {', '.join(map(str, entity_ids))}\")\n",
    "            # print(\"-------------------------------------------------------------------\")\n",
    "            current_str['rec'].extend(entity_ids)\n",
    "\n",
    "            does_movie_exist = True\n",
    "        else:\n",
    "            does_movie_exist = False\n",
    "    else:\n",
    "        does_movie_exist = False\n",
    "    \n",
    "    with open(save_path, 'w') as outfile:\n",
    "        jout = json.dumps(current_str)\n",
    "        outfile.write(jout)\n",
    "    \n",
    "    return does_movie_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edge_index', 'edge_type'] []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fb7cff002847a78d53b7dc8f0ab60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/accelerate/accelerator.py:604: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
      "  warnings.warn(\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42292ab6b4be455297db6a7de1cad91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:04<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-24 02:39:54,313] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiendc/projects/.conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/thiendc/projects/.conda/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Generated response: ['System: Yes, it is. What kind of movies do you like? Have a good day! You are welcome and have fun with the guy who did <movie>. It was really funny how about <movie>? That one has an actor named Michael Caine as well if anyone wants more comedy they should check out that film called The Great Detective<|endoftext|>']\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W924 02:39:57.046159946 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'System: Yes, it is. What kind of movies do you like? Have a good day! You are welcome and have fun with the guy who did <movie>. It was really funny how about <movie>? That one has an actor named Michael Caine as well if anyone wants more comedy they should check out that film called The Great Detective<|endoftext|>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_conv_module = None\n",
    "\n",
    "def initialize_conv_module():\n",
    "    global global_conv_module\n",
    "    if global_conv_module is None:\n",
    "        global_conv_module = GenerateConversation()\n",
    "initialize_conv_module()\n",
    "global_conv_module.generate_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "Movies that you went with earlier: spider-man 3, spider-man 2, spider-man, spider, the amazing spider-man, the amazing spider-man 2, spider-man: homecoming, spider-man strikes back, 9-man\n",
      "Corresponding IDs : 1527, 4771, 22449, 20442, 7525, 29481, 5137, 17787, 28225\n",
      "***************\n",
      "Does entity exist: True\n",
      "***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edge_index', 'edge_type'] []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d281c91c10f4134b05d4bb51134d8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/accelerate/accelerator.py:604: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/thiendc/projects/.conda/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/thiendc/projects/.conda/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'dataset_conv'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m initialize_conv_module()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Use the global conv_module\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m predict_conversation \u001b[38;5;241m=\u001b[39m \u001b[43mglobal_conv_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_conversations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mĐây là conversation\u001b[39m\u001b[38;5;124m'\u001b[39m, predict_conversation) \u001b[38;5;66;03m# list\u001b[39;00m\n\u001b[1;32m     16\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/redial/sample_input_data_processed.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/InferConverRec/src/generator/conv.py:207\u001b[0m, in \u001b[0;36mGenerateConversation.generate_conversations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_encoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_local_main_process\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    208\u001b[0m         token_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    209\u001b[0m         prompt_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_encoder(\n\u001b[1;32m    210\u001b[0m             entity_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    211\u001b[0m             token_embeds\u001b[38;5;241m=\u001b[39mtoken_embeds,\n\u001b[1;32m    212\u001b[0m             output_entity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    213\u001b[0m             use_conv_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    214\u001b[0m         )\n",
      "File \u001b[0;32m~/projects/.conda/lib/python3.11/site-packages/tqdm/notebook.py:223\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m colour \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    222\u001b[0m display_here \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgui\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39m_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m__: \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/.conda/lib/python3.11/site-packages/tqdm/asyncio.py:33\u001b[0m, in \u001b[0;36mtqdm_asyncio.__init__\u001b[0;34m(self, iterable, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterable_next \u001b[38;5;241m=\u001b[39m iterable\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__next__\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterable_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterable_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterable_iterator\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__next__\u001b[39m\n",
      "File \u001b[0;32m~/projects/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1038\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1031\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/projects/.conda/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/.conda/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/.conda/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/.conda/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/.conda/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/.conda/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_str =  input()\n",
    "global global_conv_module\n",
    "doesEntityExist = input2jsonl(input_str, entity2id_json = './data/redial_gen/entity2id.json')    \n",
    "\n",
    "print(\"***************\")\n",
    "print(\"Does entity exist:\", doesEntityExist)\n",
    "print(\"***************\")\n",
    "\n",
    "if doesEntityExist:\n",
    "    # Initialize conv_module if it hasn't been initialized yet\n",
    "    initialize_conv_module()\n",
    "    # Use the global conv_module\n",
    "    predict_conversation = global_conv_module.generate_conversations()\n",
    "    print('Đây là conversation', predict_conversation) # list\n",
    "\n",
    "    save_path = './data/redial/sample_input_data_processed.jsonl'\n",
    "    try:\n",
    "        with open(save_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "    except json.JSONDecodeError:\n",
    "        # If the file is empty or invalid JSON, create a new dictionary\n",
    "        data = {\"context\": [], \"resp\": \"\", \"rec\": [], \"entity\": []}\n",
    "\n",
    "    if isinstance(predict_conversation, list):\n",
    "        predict_conversation = ' '.join(predict_conversation)\n",
    "\n",
    "    predict_conversation = rewrite2(predict_conversation)\n",
    "    predict_conversation = predict_conversation.replace(\"<|endoftext|>\", \"\")\n",
    "    predict_conversation = predict_conversation.replace(\"System: \", \"\")\n",
    "    \n",
    "    if not any(q in predict_conversation for q in question_type):\n",
    "        if any(word in ['yes', 'no'] for word in predict_conversation.split(\".\")):\n",
    "            predict_conversation = \".\".join([i for i in predict_conversation.split(\".\") if 'yes' not in i and 'no' not in i])\n",
    "\n",
    "    if predict_conversation.strip() == \"\":\n",
    "        predict_conversation = random.choice(hook_sentences)\n",
    "    \n",
    "    if is_valid_sentence(predict_conversation):\n",
    "        predict_conversation = rewrite(predict_conversation)\n",
    "\n",
    "    data['resp'] = predict_conversation\n",
    "\n",
    "    with open(save_path, 'w') as json_file:\n",
    "        json.dump(data, json_file)\n",
    "\n",
    "    \n",
    "    subprocess.run([\n",
    "        \"accelerate\", \"launch\", \n",
    "        \"--num_processes\", \"2\",  # Chính xác 4 processes cho 4 GPU\n",
    "        \"infer_rec.py\",\n",
    "        \"--dataset\", \"redial_gen\",\n",
    "        \"--tokenizer\", \"/home/thiendc/InferConverRec/src/utils/dialogpt\",\n",
    "        \"--model\", \"/home/thiendc/InferConverRec/src/utils/dialogpt_model\",\n",
    "        \"--text_tokenizer\", \"/home/thiendc/InferConverRec/src/utils/roberta\",\n",
    "        \"--text_encoder\", \"/home/thiendc/InferConverRec/src/utils/roberta_model\",\n",
    "        \"--n_prefix_rec\", \"20\",\n",
    "        \"--prompt_encoder\", \"/home/thiendc/InferConverRec/src/output_dir/rec1/best\",\n",
    "        \"--per_device_eval_batch_size\", \"16\",\n",
    "        \"--gradient_accumulation_steps\", \"2\",\n",
    "        \"--context_max_length\", \"128\",\n",
    "        \"--prompt_max_length\", \"128\",\n",
    "        \"--entity_max_length\", \"32\",\n",
    "    ], check=True)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gt': 5831, 'recommendation': [2796, 7246, 29492, 12603, 24963, 5137, 15145, 20216, 14653, 20490, 20212, 28721, 20444, 6401, 4118, 16056, 20149, 7683, 16740, 22449, 20827, 27093, 10573, 11803, 7701, 1738, 7692, 9636, 13425, 372, 15370, 20394, 15063, 19346, 20266, 9457, 16847, 28647, 30737, 12753, 2547, 28693, 28205, 1113, 6604, 17098, 16688, 3429, 8061, 9277]}\n",
      "{'gt': 29492, 'recommendation': [2796, 7246, 29492, 12603, 24963, 5137, 15145, 20216, 14653, 20490, 20212, 28721, 20444, 6401, 4118, 16056, 20149, 7683, 16740, 22449, 20827, 27093, 10573, 11803, 7701, 1738, 7692, 9636, 13425, 372, 15370, 20394, 15063, 19346, 20266, 9457, 16847, 28647, 30737, 12753, 2547, 28693, 28205, 1113, 6604, 17098, 16688, 3429, 8061, 9277]}\n",
      "{'gt': 2216, 'recommendation': [2796, 7246, 29492, 12603, 24963, 5137, 15145, 20216, 14653, 20490, 20212, 28721, 20444, 6401, 4118, 16056, 20149, 7683, 16740, 22449, 20827, 27093, 10573, 11803, 7701, 1738, 7692, 9636, 13425, 372, 15370, 20394, 15063, 19346, 20266, 9457, 16847, 28647, 30737, 12753, 2547, 28693, 28205, 1113, 6604, 17098, 16688, 3429, 8061, 9277]}\n",
      "{'gt': 5831, 'recommendation': [7246, 2796, 29492, 24963, 20490, 20216, 5137, 12603, 15145, 14653, 20212, 20444, 28721, 7692, 16740, 1738, 6401, 4118, 16056, 15370, 7683, 11803, 22449, 20827, 13425, 27093, 20149, 30737, 7701, 10573, 9636, 12753, 372, 28205, 20266, 29510, 19346, 2547, 28908, 28647, 1113, 28693, 15063, 20394, 17098, 7423, 16847, 9277, 11048, 6604]}\n",
      "{'gt': 29492, 'recommendation': [7246, 2796, 29492, 24963, 20490, 20216, 5137, 12603, 15145, 14653, 20212, 20444, 28721, 7692, 16740, 1738, 6401, 4118, 16056, 15370, 7683, 11803, 22449, 20827, 13425, 27093, 20149, 30737, 7701, 10573, 9636, 12753, 372, 28205, 20266, 29510, 19346, 2547, 28908, 28647, 1113, 28693, 15063, 20394, 17098, 7423, 16847, 9277, 11048, 6604]}\n",
      "{'gt': 2216, 'recommendation': [7246, 2796, 29492, 24963, 20490, 20216, 5137, 12603, 15145, 14653, 20212, 20444, 28721, 7692, 16740, 1738, 6401, 4118, 16056, 15370, 7683, 11803, 22449, 20827, 13425, 27093, 20149, 30737, 7701, 10573, 9636, 12753, 372, 28205, 20266, 29510, 19346, 2547, 28908, 28647, 1113, 28693, 15063, 20394, 17098, 7423, 16847, 9277, 11048, 6604]}\n",
      "{'gt': 5831, 'recommendation': [2796, 7246, 29492, 12603, 24963, 5137, 15145, 20216, 14653, 20490, 20212, 28721, 20444, 6401, 4118, 16056, 20149, 7683, 16740, 22449, 20827, 27093, 10573, 11803, 7701, 1738, 7692, 9636, 13425, 372, 15370, 20394, 15063, 19346, 20266, 9457, 16847, 28647, 30737, 12753, 2547, 28693, 28205, 1113, 6604, 17098, 16688, 3429, 8061, 9277]}\n",
      "{'gt': 29492, 'recommendation': [2796, 7246, 29492, 12603, 24963, 5137, 15145, 20216, 14653, 20490, 20212, 28721, 20444, 6401, 4118, 16056, 20149, 7683, 16740, 22449, 20827, 27093, 10573, 11803, 7701, 1738, 7692, 9636, 13425, 372, 15370, 20394, 15063, 19346, 20266, 9457, 16847, 28647, 30737, 12753, 2547, 28693, 28205, 1113, 6604, 17098, 16688, 3429, 8061, 9277]}\n",
      "{'gt': 2216, 'recommendation': [2796, 7246, 29492, 12603, 24963, 5137, 15145, 20216, 14653, 20490, 20212, 28721, 20444, 6401, 4118, 16056, 20149, 7683, 16740, 22449, 20827, 27093, 10573, 11803, 7701, 1738, 7692, 9636, 13425, 372, 15370, 20394, 15063, 19346, 20266, 9457, 16847, 28647, 30737, 12753, 2547, 28693, 28205, 1113, 6604, 17098, 16688, 3429, 8061, 9277]}\n",
      "{'gt': 5831, 'recommendation': [7246, 2796, 29492, 24963, 20490, 20216, 5137, 12603, 15145, 14653, 20212, 20444, 28721, 7692, 16740, 1738, 6401, 4118, 16056, 15370, 7683, 11803, 22449, 20827, 13425, 27093, 20149, 30737, 7701, 10573, 9636, 12753, 372, 28205, 20266, 29510, 19346, 2547, 28908, 28647, 1113, 28693, 15063, 20394, 17098, 7423, 16847, 9277, 11048, 6604]}\n",
      "{'gt': 29492, 'recommendation': [7246, 2796, 29492, 24963, 20490, 20216, 5137, 12603, 15145, 14653, 20212, 20444, 28721, 7692, 16740, 1738, 6401, 4118, 16056, 15370, 7683, 11803, 22449, 20827, 13425, 27093, 20149, 30737, 7701, 10573, 9636, 12753, 372, 28205, 20266, 29510, 19346, 2547, 28908, 28647, 1113, 28693, 15063, 20394, 17098, 7423, 16847, 9277, 11048, 6604]}\n",
      "{'gt': 2216, 'recommendation': [7246, 2796, 29492, 24963, 20490, 20216, 5137, 12603, 15145, 14653, 20212, 20444, 28721, 7692, 16740, 1738, 6401, 4118, 16056, 15370, 7683, 11803, 22449, 20827, 13425, 27093, 20149, 30737, 7701, 10573, 9636, 12753, 372, 28205, 20266, 29510, 19346, 2547, 28908, 28647, 1113, 28693, 15063, 20394, 17098, 7423, 16847, 9277, 11048, 6604]}\n",
      "{'gt': 5831, 'recommendation': [2796, 7246, 29492, 12603, 24963, 5137, 15145, 20216, 14653, 20490, 20212, 28721, 20444, 6401, 4118, 16056, 20149, 7683, 16740, 22449, 20827, 27093, 10573, 11803, 7701, 1738, 7692, 9636, 13425, 372, 15370, 20394, 15063, 19346, 20266, 9457, 16847, 28647, 30737, 12753, 2547, 28693, 28205, 1113, 6604, 17098, 16688, 3429, 8061, 9277]}\n",
      "{'gt': 29492, 'recommendation': [2796, 7246, 29492, 12603, 24963, 5137, 15145, 20216, 14653, 20490, 20212, 28721, 20444, 6401, 4118, 16056, 20149, 7683, 16740, 22449, 20827, 27093, 10573, 11803, 7701, 1738, 7692, 9636, 13425, 372, 15370, 20394, 15063, 19346, 20266, 9457, 16847, 28647, 30737, 12753, 2547, 28693, 28205, 1113, 6604, 17098, 16688, 3429, 8061, 9277]}\n",
      "{'gt': 2216, 'recommendation': [2796, 7246, 29492, 12603, 24963, 5137, 15145, 20216, 14653, 20490, 20212, 28721, 20444, 6401, 4118, 16056, 20149, 7683, 16740, 22449, 20827, 27093, 10573, 11803, 7701, 1738, 7692, 9636, 13425, 372, 15370, 20394, 15063, 19346, 20266, 9457, 16847, 28647, 30737, 12753, 2547, 28693, 28205, 1113, 6604, 17098, 16688, 3429, 8061, 9277]}\n",
      "{'gt': 5831, 'recommendation': [7246, 2796, 29492, 24963, 20490, 20216, 5137, 12603, 15145, 14653, 20212, 20444, 28721, 7692, 16740, 1738, 6401, 4118, 16056, 15370, 7683, 11803, 22449, 20827, 13425, 27093, 20149, 30737, 7701, 10573, 9636, 12753, 372, 28205, 20266, 29510, 19346, 2547, 28908, 28647, 1113, 28693, 15063, 20394, 17098, 7423, 16847, 9277, 11048, 6604]}\n"
     ]
    }
   ],
   "source": [
    "with open('./data/redial_gen/movie_ids.json', 'r') as moviesl:\n",
    "    list_movies = json.load(moviesl)\n",
    "\n",
    "with open('./recommedations.json', 'r') as f:\n",
    "    movie_rec = json.load(f)\n",
    "\n",
    "recommendation_counts = defaultdict(int)\n",
    "\n",
    "# Duyệt qua từng item trong movie_rec và đếm số lần xuất hiện của mỗi recommendation\n",
    "for item in movie_rec:\n",
    "    recommendations = item[0]['recommendation']\n",
    "    for rec in recommendations:\n",
    "        recommendation_counts[rec] += 1\n",
    "\n",
    "# Sắp xếp các recommendation dựa trên số lần xuất hiện (tần suất) và tạo list unique gt\n",
    "sorted_recommendations = sorted(recommendation_counts.keys(), key=lambda x: recommendation_counts[x], reverse=True)\n",
    "\n",
    "# Nhóm các unique 'gt' vào list\n",
    "unique_gt = list({item[0]['gt'] for item in movie_rec})\n",
    "\n",
    "# In kết quả\n",
    "print(\"Unique GTs:\", unique_gt)\n",
    "print(\"Ranked Recommendations:\", sorted_recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique GTs: [2216, 29492, 5831]\n",
      "Ranked Recommendations: [2796, 7246, 29492, 12603, 24963, 5137, 15145, 20216, 14653, 20490, 20212, 28721, 20444, 6401, 4118, 16056, 20149, 7683, 16740, 22449, 20827, 27093, 10573, 11803, 7701, 1738, 7692, 9636, 13425, 372, 15370, 20394, 15063, 19346, 20266, 16847, 28647, 30737, 12753, 2547, 28693, 28205, 1113, 6604, 17098, 9277, 9457, 16688, 3429, 8061, 29510, 28908, 7423, 11048]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Tạo cấu trúc defaultdict để lưu số lần xuất hiện của các recommendation\n",
    "recommendation_counts = defaultdict(int)\n",
    "\n",
    "# Duyệt qua từng item trong movie_rec và đếm số lần xuất hiện của mỗi recommendation\n",
    "for item in movie_rec:\n",
    "    recommendations = item[0]['recommendation']\n",
    "    for rec in recommendations:\n",
    "        recommendation_counts[rec] += 1\n",
    "\n",
    "# Sắp xếp các recommendation dựa trên số lần xuất hiện (tần suất) và tạo list unique gt\n",
    "sorted_recommendations = sorted(recommendation_counts.keys(), key=lambda x: recommendation_counts[x], reverse=True)\n",
    "# Nhóm các unique 'gt' vào list\n",
    "unique_gt = list({item[0]['gt'] for item in movie_rec})\n",
    "\n",
    "# In kết quả\n",
    "print(\"Unique GTs:\", unique_gt)\n",
    "print(\"Ranked Recommendations:\", sorted_recommendations)\n",
    "\n",
    "if predict_conversation.count(\"<movie>\") >= 1:\n",
    "    data['rec'] = sorted_recommendations[:predict_conversation.count(\"<movie>\")]\n",
    "else:\n",
    "    data['rec'] = sorted_recommendations[:3]\n",
    "\n",
    "response = data['resp']\n",
    "recommended_movies = data['rec']\n",
    "\n",
    "if \"<movie>\" in response:\n",
    "    for movie_id in recommended_movies:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/redial_gen/movie_ids.json', 'r') as moviesl:\n",
    "    list_movies = json.load(moviesl)\n",
    "with open('./recommedations.json', 'r') as f:\n",
    "    movie_rec = json.load(f)\n",
    "movie_rec = movie_rec[-1][0]\n",
    "data['rec'] = [i for i in movie_rec['recommendation'] if i in list_movies][:predict_conversation.count(\"<movie>\")]\n",
    "response = data['resp']\n",
    "recommended_movies = data['rec']\n",
    "\n",
    "with open('/home/thiendc/InferConverRec/src/data/redial/entity2id.json', 'r') as f:\n",
    "    movielink2id =  json.load(f)\n",
    "id2movielink = {v: k for k, v in movielink2id.items()}\n",
    "\n",
    "for movie_id in recommended_movies:\n",
    "    if '<movie>' in response:\n",
    "        if isinstance(movie_id, list):\n",
    "            movie_id = movie_id[0]  # Take the first item if it's a list\n",
    "        movie_link = id2movielink.get(movie_id, str(movie_id))  # Use get() with a default value\n",
    "        movie_name = movie_link.rstrip(\"/\").split('/')[-1].strip('>').replace(\"_\", \" \")\n",
    "        response = response.replace(\"<movie>\", movie_name, 1)  # Replace only one occurrence\n",
    "    else:\n",
    "        response = response + \" \" + random.choice(promotional_sentences)\n",
    "        movie_link = id2movielink.get(movie_id, str(movie_id))\n",
    "        movie_name = movie_link.rstrip(\"/\").split('/')[-1].strip('>').replace(\"_\", \" \")\n",
    "        response = response + \" \" + movie_name + \".\"\n",
    "\n",
    "with open('/home/thiendc/InferConverRec/src/data/redial/sample_input_data_processed.jsonl', 'r') as file:\n",
    "    line = file.readline().strip()\n",
    "data = json.loads(line)\n",
    "data['context'].append(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
