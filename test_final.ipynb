{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thiendc/InferConverRec/src\n"
     ]
    }
   ],
   "source": [
    "%cd src\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "from collections import defaultdict\n",
    "from accelerate import Accelerator\n",
    "import json\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from refinement import *\n",
    "from generator.conv import GenerateConversation\n",
    "from generator.rec import GenerateRecommendation\n",
    "from generator.hooks import hook_sentences ,promotional_sentences, pick_verbs, question_type\n",
    "from fuzzywuzzy import fuzz\n",
    "multiprocessing.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edge_index', 'edge_type'] []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9cdba92692489b843dee5933f8c3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/accelerate/accelerator.py:604: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
      "  warnings.warn(\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a8fa04261b487fbc2b7d08ea8793ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:04<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-25 04:28:39,378] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiendc/projects/.conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/thiendc/projects/.conda/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Generated response: ['System: How about <movie>? It has Tom Hanks and he plays him for awhile. He was great! Did you see <movie> too? That one seems to be more recent than other ones.<|endoftext|>']\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W925 04:28:46.670225018 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'System: How about <movie>? It has Tom Hanks and he plays him for awhile. He was great! Did you see <movie> too? That one seems to be more recent than other ones.<|endoftext|>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_conv_module = None\n",
    "\n",
    "def initialize_conv_module():\n",
    "    global global_conv_module\n",
    "    if global_conv_module is None:\n",
    "        global_conv_module = GenerateConversation()\n",
    "initialize_conv_module()\n",
    "global_conv_module.generate_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities_and_ids(json_file_path):\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    entity_id = {}\n",
    "    id_entity = {}\n",
    "    for k, v in data.items():\n",
    "        k = k.split(\"/\")[-1]\n",
    "        k = k.replace(\">\", \"\")\n",
    "        # if \"(\" and \")\" in k:\n",
    "        #     k=re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", k)\n",
    "        k = k.replace(\"_\", \" \")\n",
    "        k = k.strip()\n",
    "        # k = k.lower()\n",
    "\n",
    "        entity_id[k] = v\n",
    "        id_entity[v] = k\n",
    "\n",
    "    return entity_id\n",
    "\n",
    "def cosine_similarity_entities(entities):\n",
    "    matched_entities = []\n",
    "    entity_names = list(get_entities_and_ids('./data/redial_gen/entity2id.json').keys())\n",
    "    \n",
    "    # Tạo vector TF-IDF cho tất cả thực thể trong entity_names và entities\n",
    "    vectorizer = TfidfVectorizer().fit(entity_names + entities)\n",
    "    \n",
    "    # Vector hóa danh sách entity\n",
    "    entity_vectors = vectorizer.transform(entities)\n",
    "    entity_name_vectors = vectorizer.transform(entity_names)\n",
    "\n",
    "    # Duyệt qua từng thực thể\n",
    "    for entity_vector in entity_vectors:\n",
    "        # Tính cosine similarity giữa entity hiện tại và tất cả entity_names\n",
    "        similarities = cosine_similarity(entity_vector, entity_name_vectors).flatten()\n",
    "\n",
    "        # Sắp xếp theo độ tương đồng giảm dần\n",
    "        sorted_similarities = sorted(zip(entity_names, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Lấy top 10 kết quả có độ tương đồng > 0.5 (50%)\n",
    "        top_matches = [entity_name for entity_name, similarity in sorted_similarities[:10] if similarity > 0.6]\n",
    "        matched_entities.extend(top_matches)\n",
    "\n",
    "    return matched_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input2jsonl(input_str, entity2id_json):\n",
    "    save_path = './data/redial/sample_input_data_processed.jsonl'\n",
    "    is_exist = os.path.exists(save_path)\n",
    "\n",
    "    current_str = ''\n",
    "\n",
    "    if is_exist:\n",
    "        with open(save_path, 'r') as json_file:\n",
    "            current_str = json.load(json_file)\n",
    "            current_str['context'].append(input_str)\n",
    "    else:\n",
    "        current_str = {\"context\": [input_str], \"resp\": \"\", \"rec\": [], \"entity\": []}\n",
    "    \n",
    "    entities = re.findall(r'\\$.*?\\$', input_str)\n",
    "    print(entities)\n",
    "    if entities:\n",
    "        entities = [entity.replace('$', '').lower() for entity in entities]\n",
    "        # list_entities = set(get_entities_and_ids(entity2id_json))\n",
    "\n",
    "        matched_entities = cosine_similarity_entities(entities)\n",
    "        print('Đây là các thực thể được lựa chọn', matched_entities)\n",
    "        entity_id = get_entities_and_ids('./data/redial_gen/entity2id.json')\n",
    "        if matched_entities:\n",
    "            entity_ids = [entity_id[entity] for entity in matched_entities if entity in get_entities_and_ids(entity2id_json)]\n",
    "            print(\"-------------------------------------------------------------------\")\n",
    "            print(f\"Movies that you {random.choice(pick_verbs)} earlier: {', '.join(matched_entities)}\")\n",
    "            print(f\"Corresponding IDs : {', '.join(map(str, entity_ids))}\")\n",
    "            # print(\"-------------------------------------------------------------------\")\n",
    "            current_str['rec'].extend(entity_ids)\n",
    "\n",
    "            does_movie_exist = True\n",
    "        else:\n",
    "            does_movie_exist = False\n",
    "    else:\n",
    "        does_movie_exist = False\n",
    "    \n",
    "    with open(save_path, 'w') as outfile:\n",
    "        jout = json.dumps(current_str)\n",
    "        outfile.write(jout)\n",
    "    \n",
    "    return does_movie_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edge_index', 'edge_type'] []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8d7fb949c346aba0aac9cbb601ef0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/accelerate/accelerator.py:604: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c7ef6d3da440c781eb7469c6ff05bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:04<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-25 03:56:35,705] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiendc/projects/.conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/thiendc/projects/.conda/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `0.8` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Generated response: ['System: I have a few more movies.<|endoftext|>']\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W925 03:56:38.872671025 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'System: I have a few more movies.<|endoftext|>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_conv_module = None\n",
    "\n",
    "def initialize_conv_module():\n",
    "    global global_conv_module\n",
    "    if global_conv_module is None:\n",
    "        global_conv_module = GenerateConversation()\n",
    "initialize_conv_module()\n",
    "global_conv_module.generate_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'I want to watch movies like $The Avengers$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$The Avengers$']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7fbff098c680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thiendc/projects/.conda/lib/python3.11/multiprocessing/connection.py\", line 133, in __del__\n",
      "    self._close()\n",
      "  File \"/home/thiendc/projects/.conda/lib/python3.11/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đây là các thực thể được lựa chọn ['3 Avengers', 'The Avengers (2012 film)', 'The Avengers (1998 film)', 'Avengers Grimm']\n",
      "-------------------------------------------------------------------\n",
      "Movies that you preferred earlier: 3 Avengers, The Avengers (2012 film), The Avengers (1998 film), Avengers Grimm\n",
      "Corresponding IDs : 5831, 29492, 2216, 5794\n",
      "***************\n",
      "Does entity exist: True\n",
      "***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f51ecde6a3e424c91d88718f90e40da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:04<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/thiendc/projects/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Generated response: ['System: Yes. Have a good day! Bye! Goodbye and enjoy your time with the new film! bye. Goodnight u too<|endoftext|>']\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W924 04:27:45.744747182 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đây là conversation System: Yes. Have a good day! Bye! Goodbye and enjoy your time with the new film! bye. Goodnight u too<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/thiendc/projects/.conda/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/thiendc/projects/.conda/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
      "    args.func(args)\n",
      "  File \"/home/thiendc/projects/.conda/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1097, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/home/thiendc/projects/.conda/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 712, in multi_gpu_launcher\n",
      "    current_env = prepare_multi_gpu_env(args)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/thiendc/projects/.conda/lib/python3.11/site-packages/accelerate/utils/launch.py\", line 193, in prepare_multi_gpu_env\n",
      "    raise ConnectionError(\n",
      "ConnectionError: Tried to launch distributed communication on port `29500`, but another process is utilizing it. Please specify a different port (such as using the `--main_process_port` flag or specifying a different `main_process_port` in your config file) and rerun your script. To automatically use the next open port (on a single node), you can set this to `0`.\n",
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7fbff098c680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thiendc/projects/.conda/lib/python3.11/multiprocessing/connection.py\", line 133, in __del__\n",
      "    self._close()\n",
      "  File \"/home/thiendc/projects/.conda/lib/python3.11/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['accelerate', 'launch', '--num_processes', '2', 'infer_rec.py', '--dataset', 'redial_gen', '--tokenizer', '/home/thiendc/InferConverRec/src/utils/dialogpt', '--model', '/home/thiendc/InferConverRec/src/utils/dialogpt_model', '--text_tokenizer', '/home/thiendc/InferConverRec/src/utils/roberta', '--text_encoder', '/home/thiendc/InferConverRec/src/utils/roberta_model', '--n_prefix_rec', '20', '--prompt_encoder', '/home/thiendc/InferConverRec/src/output_dir/rec1/best', '--per_device_eval_batch_size', '16', '--gradient_accumulation_steps', '2', '--context_max_length', '128', '--prompt_max_length', '128', '--entity_max_length', '32']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m     44\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(data, json_file)\n\u001b[0;32m---> 47\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccelerate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlaunch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--num_processes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Chính xác 4 processes cho 4 GPU\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minfer_rec.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mredial_gen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--tokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/thiendc/InferConverRec/src/utils/dialogpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/thiendc/InferConverRec/src/utils/dialogpt_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--text_tokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/thiendc/InferConverRec/src/utils/roberta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--text_encoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/thiendc/InferConverRec/src/utils/roberta_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--n_prefix_rec\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m20\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--prompt_encoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/thiendc/InferConverRec/src/output_dir/rec1/best\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--per_device_eval_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--gradient_accumulation_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--context_max_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m128\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--prompt_max_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m128\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--entity_max_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/redial_gen/movie_ids.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m moviesl:\n\u001b[1;32m     66\u001b[0m     list_movies \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(moviesl)\n",
      "File \u001b[0;32m~/projects/.conda/lib/python3.11/subprocess.py:571\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 571\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    572\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['accelerate', 'launch', '--num_processes', '2', 'infer_rec.py', '--dataset', 'redial_gen', '--tokenizer', '/home/thiendc/InferConverRec/src/utils/dialogpt', '--model', '/home/thiendc/InferConverRec/src/utils/dialogpt_model', '--text_tokenizer', '/home/thiendc/InferConverRec/src/utils/roberta', '--text_encoder', '/home/thiendc/InferConverRec/src/utils/roberta_model', '--n_prefix_rec', '20', '--prompt_encoder', '/home/thiendc/InferConverRec/src/output_dir/rec1/best', '--per_device_eval_batch_size', '16', '--gradient_accumulation_steps', '2', '--context_max_length', '128', '--prompt_max_length', '128', '--entity_max_length', '32']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "input_str =  input()\n",
    "global global_conv_module\n",
    "doesEntityExist = input2jsonl(input_str, entity2id_json = './data/redial_gen/entity2id.json')    \n",
    "\n",
    "print(\"***************\")\n",
    "print(\"Does entity exist:\", doesEntityExist)\n",
    "print(\"***************\")\n",
    "\n",
    "if doesEntityExist:\n",
    "    # Initialize conv_module if it hasn't been initialized yet\n",
    "    initialize_conv_module()\n",
    "    # Use the global conv_module\n",
    "    predict_conversation = global_conv_module.generate_conversations()\n",
    "    print('Đây là conversation', predict_conversation) # list\n",
    "\n",
    "    save_path = './data/redial/sample_input_data_processed.jsonl'\n",
    "    try:\n",
    "        with open(save_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "    except json.JSONDecodeError:\n",
    "        # If the file is empty or invalid JSON, create a new dictionary\n",
    "        data = {\"context\": [], \"resp\": \"\", \"rec\": [], \"entity\": []}\n",
    "\n",
    "    if isinstance(predict_conversation, list):\n",
    "        predict_conversation = ' '.join(predict_conversation)\n",
    "\n",
    "    predict_conversation = rewrite2(predict_conversation)\n",
    "    predict_conversation = predict_conversation.replace(\"<|endoftext|>\", \"\")\n",
    "    predict_conversation = predict_conversation.replace(\"System: \", \"\")\n",
    "    \n",
    "    if not any(q in predict_conversation for q in question_type):\n",
    "        if any(word in ['yes', 'no'] for word in predict_conversation.split(\".\")):\n",
    "            predict_conversation = \".\".join([i for i in predict_conversation.split(\".\") if 'yes' not in i and 'no' not in i])\n",
    "\n",
    "    if predict_conversation.strip() == \"\":\n",
    "        predict_conversation = random.choice(hook_sentences)\n",
    "    \n",
    "    if is_valid_sentence(predict_conversation):\n",
    "        predict_conversation = rewrite(predict_conversation)\n",
    "\n",
    "    data['resp'] = predict_conversation\n",
    "\n",
    "    with open(save_path, 'w') as json_file:\n",
    "        json.dump(data, json_file)\n",
    "\n",
    "    \n",
    "    subprocess.run([\n",
    "        \"accelerate\", \"launch\", \n",
    "        \"--num_processes\", \"2\",  # Chính xác 4 processes cho 4 GPU\n",
    "        \"infer_rec.py\",\n",
    "        \"--dataset\", \"redial_gen\",\n",
    "        \"--tokenizer\", \"/home/thiendc/InferConverRec/src/utils/dialogpt\",\n",
    "        \"--model\", \"/home/thiendc/InferConverRec/src/utils/dialogpt_model\",\n",
    "        \"--text_tokenizer\", \"/home/thiendc/InferConverRec/src/utils/roberta\",\n",
    "        \"--text_encoder\", \"/home/thiendc/InferConverRec/src/utils/roberta_model\",\n",
    "        \"--n_prefix_rec\", \"20\",\n",
    "        \"--prompt_encoder\", \"/home/thiendc/InferConverRec/src/output_dir/rec1/best\",\n",
    "        \"--per_device_eval_batch_size\", \"16\",\n",
    "        \"--gradient_accumulation_steps\", \"2\",\n",
    "        \"--context_max_length\", \"128\",\n",
    "        \"--prompt_max_length\", \"128\",\n",
    "        \"--entity_max_length\", \"32\",\n",
    "    ], check=True)\n",
    "\n",
    "    with open('./data/redial_gen/movie_ids.json', 'r') as moviesl:\n",
    "        list_movies = json.load(moviesl)\n",
    "\n",
    "    with open('./recommedations.json', 'r') as f:\n",
    "        movie_rec = json.load(f)\n",
    "\n",
    "    recommendation_counts = defaultdict(int)\n",
    "\n",
    "    # Duyệt qua từng item trong movie_rec và đếm số lần xuất hiện của mỗi recommendation\n",
    "    for item in movie_rec:\n",
    "        recommendations = item[0]['recommendation']\n",
    "        for rec in recommendations:\n",
    "            recommendation_counts[rec] += 1\n",
    "\n",
    "    # Sắp xếp các recommendation dựa trên số lần xuất hiện (tần suất) và tạo list unique gt\n",
    "    sorted_recommendations = sorted(recommendation_counts.keys(), key=lambda x: recommendation_counts[x], reverse=True)\n",
    "    # Nhóm các unique 'gt' vào list\n",
    "    unique_gt = list({item[0]['gt'] for item in movie_rec})\n",
    "\n",
    "    # In kết quả\n",
    "    print(\"Unique GTs:\", unique_gt)\n",
    "    print(\"Ranked Recommendations:\", sorted_recommendations)\n",
    "\n",
    "    if predict_conversation.count(\"<movie>\") >= 1:\n",
    "        data['rec'] = sorted_recommendations[:predict_conversation.count(\"<movie>\")]\n",
    "    else:\n",
    "        data['rec'] = sorted_recommendations[:3]\n",
    "\n",
    "    response = data['resp']\n",
    "    recommended_movies = data['rec']\n",
    "\n",
    "    if \"<movie>\" in response:\n",
    "        for movie_id in recommended_movies:\n",
    "            movie_name = {v:k for k, v in get_entities_and_ids('./data/redial/entity2id.json').items()}.get(movie_id, str(movie_id))\n",
    "            response = response.replace(\"<movie>\", movie_name, 1)\n",
    "    else:\n",
    "        movie_mask = ', '.join(['<movie>'] * len(data['rec']))\n",
    "        response = f\"{response} {random.choice(promotional_sentences)} {movie_mask}\"\n",
    "    \n",
    "    print('response', response)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('./data/redial/dbpedia_subkg.json', 'r') as jsonf:\n",
    "    datas = json.load(jsonf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Match: The Avengers (2012 film)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "texts = ['3 Avengers', 'The Avengers (2012 film)', 'The Avengers (1998 film)', 'Avengers Grimm']\n",
    "input_text = \"The Avegners\"\n",
    "\n",
    "# Preprocessing (lowercasing)\n",
    "texts_lower = [text.lower() for text in texts]\n",
    "input_text = input_text.lower()\n",
    "\n",
    "# Vectorizing the texts\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(texts_lower)  # fit once for all texts\n",
    "\n",
    "# Transform the input text to vector using the same vectorizer\n",
    "input_vector = vectorizer.transform([input_text])\n",
    "\n",
    "# Compute cosine similarity\n",
    "cos_similarities = cosine_similarity(input_vector, vectors).flatten()\n",
    "\n",
    "# Handling typos with fuzzy matching\n",
    "fuzzy_scores = [fuzz.ratio(input_text, text) for text in texts_lower]\n",
    "\n",
    "# Combine cosine similarity and fuzzy score\n",
    "combined_scores = [(cosine_sim + fuzzy_score / 100) / 2 for cosine_sim, fuzzy_score in zip(cos_similarities, fuzzy_scores)]\n",
    "\n",
    "# Sort by combined score\n",
    "best_match = texts[combined_scores.index(max(combined_scores))]\n",
    "\n",
    "print(f\"Best Match: {best_match}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.365, 0.5535481022632733, 0.5535481022632733, 0.27]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
