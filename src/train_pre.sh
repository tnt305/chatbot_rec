accelerate launch train_pre.py --dataset redial --tokenizer utils/dialogpt --model utils/dialogpt_model --text_tokenizer utils/roberta --text_encoder utils/roberta_model --num_train_epochs 20  --gradient_accumulation_steps 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 64 --num_warmup_steps 168 --max_length 128 --prompt_max_length 128 --entity_max_length 32 --learning_rate 5e-5 --output_dir output_dir_inspired/prompt/dialogpt5e5_v1 --use_wandb --project conv_rec --name prompt --log_all
# accelerate launch train_pre.py --dataset redial --tokenizer utils/dialogpt --model utils/dialogpt_model --text_tokenizer utils/roberta --text_encoder utils/roberta_model --num_train_epochs 20  --gradient_accumulation_steps 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 64 --num_warmup_steps 168 --max_length 128 --prompt_max_length 128 --entity_max_length 32 --learning_rate 1e-4 --output_dir output_dir_inspired/prompt/dialogpt1e4_v1 --use_wandb --project conv_rec --name prompt --log_all
# accelerate launch train_pre.py --dataset redial --tokenizer utils/dialogpt --model utils/dialogpt_model --text_tokenizer utils/roberta --text_encoder utils/roberta_model --num_train_epochs 20  --gradient_accumulation_steps 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 64 --num_warmup_steps 168 --max_length 128 --prompt_max_length 128 --entity_max_length 32 --learning_rate 1e-3 --output_dir output_dir_inspired/prompt/dialogpt1e3_v1 --use_wandb --project conv_rec --name prompt --log_all

# accelerate launch train_pre.py --dataset inspired --tokenizer utils/dialogpt --model utils/dialogpt_model --text_tokenizer utils/roberta --text_encoder utils/roberta_model --num_train_epochs 20  --gradient_accumulation_steps 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 64 --num_warmup_steps 168 --max_length 128 --prompt_max_length 128 --entity_max_length 32 --learning_rate 5e-5 --output_dir output_dir_inspired/prompt/dialogpt5e5_v1 --use_wandb --project conv_rec --name prompt --log_all
# accelerate launch train_pre.py --dataset inspired --tokenizer utils/dialogpt --model utils/dialogpt_model --text_tokenizer utils/roberta --text_encoder utils/roberta_model --num_train_epochs 20  --gradient_accumulation_steps 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 64 --num_warmup_steps 168 --max_length 128 --prompt_max_length 128 --entity_max_length 32 --learning_rate 1e-4 --output_dir output_dir_inspired/prompt/dialogpt1e4_v1 --use_wandb --project conv_rec --name prompt --log_all
# accelerate launch train_pre.py --dataset inspired --tokenizer utils/dialogpt --model utils/dialogpt_model --text_tokenizer utils/roberta --text_encoder utils/roberta_model --num_train_epochs 20  --gradient_accumulation_steps 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 64 --num_warmup_steps 168 --max_length 128 --prompt_max_length 128 --entity_max_length 32 --learning_rate 1e-3 --output_dir output_dir_inspired/prompt/dialogpt1e3_v1 --use_wandb --project conv_rec --name prompt --log_all
